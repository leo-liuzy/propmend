{
       "cells": [
              {
                     "cell_type": "code",
                     "execution_count": 26,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "text/plain": [
                                                 "'/datastor1/zliu/mend/notebooks'"
                                          ]
                                   },
                                   "execution_count": 26,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "import pandas as pd\n",
                            "from typing import List\n",
                            "# from experiments.musique.inference_only import macro_averaging\n",
                            "import vars\n",
                            "import os\n",
                            "import numpy as np\n",
                            "from tqdm import tqdm\n",
                            "import seaborn as sns\n",
                            "import matplotlib.pyplot as plt\n",
                            "from glob import glob\n",
                            "from scipy.stats import describe\n",
                            "from thefuzz import fuzz\n",
                            "from copy import deepcopy\n",
                            "from utils import is_significantly_different, load_json, load_jsonlines\n",
                            "\n",
                            "from collections import defaultdict\n",
                            "\n",
                            "\n",
                            "def macro_averaging(df: pd.DataFrame, metrics: List[str], multi_level_averaging: List[str]):\n",
                            "    \"\"\"\n",
                            "    Do macro-averaging over the given metrics and multi-level averaging categories.\n",
                            "    \"\"\"\n",
                            "    extracted_multi_level_cols = [[m, \"mean\"] for m in metrics]\n",
                            "    while len(multi_level_averaging) > 0:\n",
                            "        # first take the mean over each generation,\n",
                            "        # and, only take `mean` of `rouge1` and  `llm_accuracy` column groups\n",
                            "        df_over_cols = df.groupby(multi_level_averaging, observed=True).describe()[extracted_multi_level_cols]\n",
                            "        # remove the multi-level column indices, since there's only one sub-level -- \"mean\"\n",
                            "        df_over_cols.columns = df_over_cols.columns.get_level_values(0)\n",
                            "\n",
                            "        # reset index to flatten the multi-level column indices for the next macro-averaging class\n",
                            "        df = df_over_cols.reset_index(inplace=False)\n",
                            "        multi_level_averaging.pop(-1)\n",
                            "    return df\n",
                            "os.getcwd()"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 149,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "Num of rows: 5223\n"
                                   ]
                            },
                            {
                                   "data": {
                                          "text/plain": [
                                                 "0"
                                          ]
                                   },
                                   "execution_count": 149,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "# df = pd.read_excel(f\"{os.getenv('PROJ_PLAYGROUND')}/mend/ripple_exp_output/llama3.2-1B-eos-sft/all/base_n=500_prompt=no_w-gen_wo-icl_ice=True.xlsx\")\n",
                            "df = pd.read_excel(\"/u/zliu/datastor1/mend/ripple_exp_output/llama3.2-1B-eos-sft/all/base_n=500_prompt=no_w-gen_wo-icl_ice=True.xlsx\")\n",
                            "print(\"Num of rows:\", len(df))\n",
                            "df.describe()[[ \"exact_match\", \"llm_accuracy\",]].round(2) # \n",
                            "pre_edit_df = df[df[\"stage\"] == \"pre-edit\"]\n",
                            "post_edit_df = df[df[\"stage\"] == \"post-edit\"]\n",
                            "len(post_edit_df)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 150,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# df.head()"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 151,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "Num of rows: 5223\n"
                                   ]
                            }
                     ],
                     "source": [
                            "base_df = pd.read_excel(f\"{os.getenv('PROJ_PLAYGROUND')}/mend/ripple_exp_output/llama3.2-1B-eos-sft/all/base_n=500_prompt=no_w-gen_wo-icl_ice=True.xlsx\")\n",
                            "print(\"Num of rows:\", len(df))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 153,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "efficacy_Logical_Generalization (n=230.0)\n",
                                          "31.7\n",
                                          "\n",
                                          "efficacy_Compositionality_I (n=1679.0)\n",
                                          "24.6\n",
                                          "\n",
                                          "efficacy_Compositionality_II (n=273.0)\n",
                                          "21.8\n",
                                          "\n",
                                          "efficacy_Subject_Aliasing (n=777.0)\n",
                                          "38.5\n",
                                          "\n",
                                          "specificity_Relation_Specificity (n=1982.0)\n",
                                          "30.3\n",
                                          "\n",
                                          "specificity_Forgetfulness (n=282.0)\n",
                                          "13.3\n",
                                          "\n"
                                   ]
                            }
                     ],
                     "source": [
                            "# df = post_edit_df\n",
                            "for tag in [\n",
                            "        \"efficacy_Logical_Generalization\",\"efficacy_Compositionality_I\",\"efficacy_Compositionality_II\",\"efficacy_Subject_Aliasing\", \"specificity_Relation_Specificity\", \"specificity_Forgetfulness\"\n",
                            "    ]:\n",
                            "    agg = df[df[\"question_tag\"] == tag].describe()[[\"llm_accuracy\",]]\n",
                            "    print(tag, f\"(n={agg['llm_accuracy']['count']})\")\n",
                            "    \n",
                            "    print((agg['llm_accuracy']['mean'] * 100).round(1)) #\n",
                            "    print()"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 154,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "text/plain": [
                                                 "1538"
                                          ]
                                   },
                                   "execution_count": 154,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "import vars\n",
                            "\n",
                            "\n",
                            "verbtaim_queries = [tuple([x[0], x[1], x[2]]) for x in load_json(f\"{vars.DATA_DIR}/ripple_edits/em_verbatim_queries_eff+spec.json\")]\n",
                            "len(verbtaim_queries)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 155,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "ripple_edits_examples = load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/test_aug.jsonl\")\n",
                            "non_zero_outerloop_count = 0\n",
                            "strong_meta_examples = []\n",
                            "weak_meta_examples = []\n",
                            "queries = []\n",
                            "for e_i, example in enumerate(ripple_edits_examples):\n",
                            "    efficacy_queries = []\n",
                            "    # non_zero_outerloop_count += len(outerloop_instances) > 0\n",
                            "    for k in [\"Logical_Generalization\", \"Compositionality_I\", \"Compositionality_II\", \"Subject_Aliasing\"]:\n",
                            "        for instance in example[k]:\n",
                            "            for q in instance[\"test_queries\"]:\n",
                            "                if (\n",
                            "                    len(q[\"answers\"]) > 0\n",
                            "                    and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]) > 0\n",
                            "                ):\n",
                            "                    q[\"question_type\"] = k\n",
                            "                    # q[\"answers\"] = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
                            "                    q[\"answers\"] = [x  for a in q[\"answers\"] for x in [a[\"value\"]] + a[\"aliases\"] if len(a[\"value\"].strip()) > 0]\n",
                            "                    \n",
                            "                    efficacy_queries.append(q)\n",
                            "    queries.extend(efficacy_queries)\n",
                            "    assert len(efficacy_queries) > 0\n",
                            "\n",
                            "    specificity_queries = []\n",
                            "    for k in [\"Relation_Specificity\", \"Forgetfulness\"]:\n",
                            "        for instance in example[k]:\n",
                            "            for q in instance[\"test_queries\"]:\n",
                            "                if (\n",
                            "                    len(q[\"answers\"]) > 0\n",
                            "                    and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]) > 0\n",
                            "                ):\n",
                            "                    q[\"question_type\"] = k\n",
                            "                    # q[\"answers\"] = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
                            "                    q[\"answers\"] = [x for a in q[\"answers\"] for x in [a[\"value\"]] + a[\"aliases\"] if len(a[\"value\"].strip()) > 0]\n",
                            "                    specificity_queries.append(q)\n",
                            "    queries.extend(specificity_queries)\n",
                            "    "
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 156,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "text/html": [
                                                 "<div>\n",
                                                 "<style scoped>\n",
                                                 "    .dataframe tbody tr th:only-of-type {\n",
                                                 "        vertical-align: middle;\n",
                                                 "    }\n",
                                                 "\n",
                                                 "    .dataframe tbody tr th {\n",
                                                 "        vertical-align: top;\n",
                                                 "    }\n",
                                                 "\n",
                                                 "    .dataframe thead th {\n",
                                                 "        text-align: right;\n",
                                                 "    }\n",
                                                 "</style>\n",
                                                 "<table border=\"1\" class=\"dataframe\">\n",
                                                 "  <thead>\n",
                                                 "    <tr style=\"text-align: right;\">\n",
                                                 "      <th></th>\n",
                                                 "      <th>id</th>\n",
                                                 "      <th>question_type</th>\n",
                                                 "      <th>question_tag</th>\n",
                                                 "      <th>relation</th>\n",
                                                 "      <th>input</th>\n",
                                                 "      <th>stage</th>\n",
                                                 "      <th>question</th>\n",
                                                 "      <th>answer</th>\n",
                                                 "      <th>predicted_answer_idx</th>\n",
                                                 "      <th>predicted_answer</th>\n",
                                                 "      <th>exact_match</th>\n",
                                                 "      <th>llm_accuracy</th>\n",
                                                 "    </tr>\n",
                                                 "  </thead>\n",
                                                 "  <tbody>\n",
                                                 "    <tr>\n",
                                                 "      <th>0</th>\n",
                                                 "      <td>0</td>\n",
                                                 "      <td>efficacy</td>\n",
                                                 "      <td>efficacy_Compositionality_II</td>\n",
                                                 "      <td>COUNTRY</td>\n",
                                                 "      <td>[[The name of the continent which United Arab ...</td>\n",
                                                 "      <td>pre-edit</td>\n",
                                                 "      <td>Imagine that the name of the continent which U...</td>\n",
                                                 "      <td>Indian Ocean</td>\n",
                                                 "      <td>0</td>\n",
                                                 "      <td>Persian Gulf</td>\n",
                                                 "      <td>0</td>\n",
                                                 "      <td>0.0</td>\n",
                                                 "    </tr>\n",
                                                 "    <tr>\n",
                                                 "      <th>1</th>\n",
                                                 "      <td>0</td>\n",
                                                 "      <td>efficacy</td>\n",
                                                 "      <td>efficacy_Compositionality_II</td>\n",
                                                 "      <td>COUNTRY</td>\n",
                                                 "      <td>[[The name of the continent which United Arab ...</td>\n",
                                                 "      <td>pre-edit</td>\n",
                                                 "      <td>Imagine that the name of the continent which U...</td>\n",
                                                 "      <td>Indian Ocean</td>\n",
                                                 "      <td>0</td>\n",
                                                 "      <td>Indian Ocean</td>\n",
                                                 "      <td>1</td>\n",
                                                 "      <td>1.0</td>\n",
                                                 "    </tr>\n",
                                                 "  </tbody>\n",
                                                 "</table>\n",
                                                 "</div>"
                                          ],
                                          "text/plain": [
                                                 "   id question_type                  question_tag relation  \\\n",
                                                 "0   0      efficacy  efficacy_Compositionality_II  COUNTRY   \n",
                                                 "1   0      efficacy  efficacy_Compositionality_II  COUNTRY   \n",
                                                 "\n",
                                                 "                                               input     stage  \\\n",
                                                 "0  [[The name of the continent which United Arab ...  pre-edit   \n",
                                                 "1  [[The name of the continent which United Arab ...  pre-edit   \n",
                                                 "\n",
                                                 "                                            question        answer  \\\n",
                                                 "0  Imagine that the name of the continent which U...  Indian Ocean   \n",
                                                 "1  Imagine that the name of the continent which U...  Indian Ocean   \n",
                                                 "\n",
                                                 "   predicted_answer_idx predicted_answer  exact_match  llm_accuracy  \n",
                                                 "0                     0     Persian Gulf            0           0.0  \n",
                                                 "1                     0     Indian Ocean            1           1.0  "
                                          ]
                                   },
                                   "execution_count": 156,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "df.head(2)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 161,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# bos = \"<|begin_of_text|>\"\n",
                            "# eos = \"<|end_of_text|>\"\n",
                            "# edit_column_name = \"edit_input\"\n",
                            "# below is for ICE and base\n",
                            "bos = \"[[\" \n",
                            "eos = \"]]\"\n",
                            "edit_column_name = \"input\""
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 170,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "ice = True\n",
                            "original_mend = False\n",
                            "memit = False"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 172,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "# efficacy: 2959\n",
                                          "# verbtaim: 1373\n",
                                          "# non-verbtaim: 1586\n"
                                   ]
                            }
                     ],
                     "source": [
                            "efficacy_df = df[(df[\"question_type\"] == \"efficacy\")] #  & (df[\"question_tag\"] == \"efficacy_Logical_Generalization\")]\n",
                            "\n",
                            "print(\"# efficacy:\", len(efficacy_df))\n",
                            "\n",
                            "# new_df_content = []\n",
                            "verbtaim_df_content = []\n",
                            "non_verbtaim_df_content = []\n",
                            "for r_i, r in efficacy_df.iterrows():\n",
                            "    # if r[\"question_type\"] == \"specificity\":\n",
                            "        # continue\n",
                            "    prop_type = r[\"question_tag\"].replace(r[\"question_type\"] + \"_\", \"\")\n",
                            "    \n",
                            "    if bos in r[edit_column_name] and eos in r[edit_column_name]:\n",
                            "        edit_input = r[edit_column_name][r[edit_column_name].index(bos) + len(bos):r[edit_column_name].index(eos)]\n",
                            "    else:\n",
                            "        edit_input = r[edit_column_name]\n",
                            "    if original_mend:\n",
                            "        edit_input += \".\"\n",
                            "    \n",
                            "    if ice:\n",
                            "        question = r[\"question\"]\n",
                            "        icl_edit = \"Imagine that \" + edit_input[0].lower() + edit_input[1:]\n",
                            "        assert icl_edit in question, question\n",
                            "        question = question.replace(icl_edit, \"\", 1).strip()\n",
                            "        \n",
                            "    else:\n",
                            "        question = r[\"question\"]\n",
                            "    \n",
                            "    assert question == queries[r_i][\"prompt\"], str(r_i) + \"@@\" + question + \"!=\" + queries[r_i][\"prompt\"]\n",
                            "    answer_full_sent = queries[r_i][\"prompt\"] + \" \" + str(r[\"predicted_answer\"])\n",
                            "    r[\"rippleedit_exact_match\"] = float(any(a in answer_full_sent for a in queries[r_i][\"answers\"]))\n",
                            "    # new_df_content.append(r)\n",
                            "    if (edit_input, prop_type, question) not in verbtaim_queries:\n",
                            "        non_verbtaim_df_content.append(r)\n",
                            "        continue\n",
                            "    # assert r[\"answer\"] in edit_input, r\n",
                            "    # if r[\"answer\"] not in edit_input:\n",
                            "    if not any(a in edit_input for a in queries[r_i][\"answers\"]):\n",
                            "        non_verbtaim_df_content.append(r)\n",
                            "        continue\n",
                            "    verbtaim_df_content.append(r)\n",
                            "\n",
                            "verbtaim_df = pd.DataFrame(verbtaim_df_content)\n",
                            "non_verbtaim_df = pd.DataFrame(non_verbtaim_df_content)\n",
                            "print(\"# verbtaim:\", len(verbtaim_df))\n",
                            "print(\"# non-verbtaim:\", len(non_verbtaim_df))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 173,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "verbtaim efficacy (llm_acc):  35.6\n",
                                          "non-verbtaim efficacy (llm_acc):  22.4\n"
                                   ]
                            }
                     ],
                     "source": [
                            "print(\"verbtaim efficacy (llm_acc): \",  (verbtaim_df.describe()[\"llm_accuracy\"][\"mean\"] * 100).round(1))\n",
                            "print(\"non-verbtaim efficacy (llm_acc): \",  (non_verbtaim_df.describe()[\"llm_accuracy\"][\"mean\"] * 100) .round(1))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 174,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "Score_A avg: 0.757\n",
                                          "Score_B avg: 0.356\n",
                                          "Delta (B - A): -0.401\n",
                                          "p: 0.0 (threshold = 0.05)\n",
                                          "Significant\n"
                                   ]
                            },
                            {
                                   "data": {
                                          "text/plain": [
                                                 "np.True_"
                                          ]
                                   },
                                   "execution_count": 174,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "is_significantly_different(\n",
                            "    propmend_verbtaim_df[\"llm_accuracy\"].values,\n",
                            "    verbtaim_df[\"llm_accuracy\"].values,\n",
                            "    alpha=0.05,\n",
                            "    verbose=True\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 175,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# propmend_verbtaim_df = verbtaim_df\n",
                            "# propmend_non_verbtaim_df = non_verbtaim_df"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 176,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "# specificity: 2264\n",
                                          "# verbtaim: 165\n",
                                          "# non-verbtaim: 2099\n"
                                   ]
                            }
                     ],
                     "source": [
                            "specificity_df = df[(df[\"question_type\"] == \"specificity\")] #  & (df[\"question_tag\"] == \"efficacy_Logical_Generalization\")]\n",
                            "\n",
                            "print(\"# specificity:\", len(specificity_df))\n",
                            "\n",
                            "# new_df_content = []\n",
                            "verbtaim_spec_df_content = []\n",
                            "non_verbtaim_spec_df_content = []\n",
                            "for r_i, r in specificity_df.iterrows():\n",
                            "    # if r[\"question_type\"] == \"specificity\":\n",
                            "        # continue\n",
                            "    prop_type = r[\"question_tag\"].replace(r[\"question_type\"] + \"_\", \"\")\n",
                            "    \n",
                            "    if bos in r[edit_column_name]:\n",
                            "        edit_input = r[edit_column_name][r[edit_column_name].index(bos) + len(bos):r[edit_column_name].index(eos)]\n",
                            "    else:\n",
                            "        edit_input = r[edit_column_name]\n",
                            "    if original_mend:\n",
                            "        edit_input += \".\"\n",
                            "    \n",
                            "    if ice:\n",
                            "        question = r[\"question\"]\n",
                            "        icl_edit = \"Imagine that \" + edit_input[0].lower() + edit_input[1:]\n",
                            "        assert icl_edit in question, question\n",
                            "        question = question.replace(icl_edit, \"\", 1).strip()\n",
                            "        \n",
                            "    else:\n",
                            "        question = r[\"question\"]\n",
                            "    \n",
                            "    assert question == queries[r_i][\"prompt\"], r_i\n",
                            "    answer_full_sent = queries[r_i][\"prompt\"] + \" \" + str(r[\"predicted_answer\"])\n",
                            "    r[\"rippleedit_exact_match\"] = float(any(a in answer_full_sent for a in queries[r_i][\"answers\"]))\n",
                            "    # new_df_content.append(r)\n",
                            "    if (edit_input, prop_type, question) not in verbtaim_queries:\n",
                            "        non_verbtaim_spec_df_content.append(r)\n",
                            "        continue\n",
                            "    # assert r[\"answer\"] in edit_input, r\n",
                            "    # if r[\"answer\"] not in edit_input:\n",
                            "    if not any(a in edit_input for a in queries[r_i][\"answers\"]):\n",
                            "        non_verbtaim_spec_df_content.append(r)\n",
                            "        continue\n",
                            "    verbtaim_spec_df_content.append(r)\n",
                            "\n",
                            "verbtaim_spec_df = pd.DataFrame(verbtaim_spec_df_content)\n",
                            "non_verbtaim_spec_df = pd.DataFrame(non_verbtaim_spec_df_content)\n",
                            "print(\"# verbtaim:\", len(verbtaim_spec_df))\n",
                            "print(\"# non-verbtaim:\", len(non_verbtaim_spec_df))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 177,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "verbtaim specificity (llm_acc):  17.8\n",
                                          "non-verbtaim specificity (llm_acc):  29.0\n"
                                   ]
                            }
                     ],
                     "source": [
                            "print(\"verbtaim specificity (llm_acc): \",  (verbtaim_spec_df.describe()[\"llm_accuracy\"][\"mean\"] * 100).round(1))\n",
                            "print(\"non-verbtaim specificity (llm_acc): \",  (non_verbtaim_spec_df.describe()[\"llm_accuracy\"][\"mean\"] * 100).round(1))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 178,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "text/plain": [
                                                 "np.float64(24.121212121212118)"
                                          ]
                                   },
                                   "execution_count": 178,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "propmend_verbtaim_spec_df.describe()[\"llm_accuracy\"][\"mean\"] * 100"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 179,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "Score_A avg: 0.241\n",
                                          "Score_B avg: 0.178\n",
                                          "Delta (B - A): -0.064\n",
                                          "p: 0.048699999999999966 (threshold = 0.05)\n",
                                          "Significant\n"
                                   ]
                            },
                            {
                                   "data": {
                                          "text/plain": [
                                                 "np.True_"
                                          ]
                                   },
                                   "execution_count": 179,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "is_significantly_different(\n",
                            "    propmend_verbtaim_spec_df[\"llm_accuracy\"].values,\n",
                            "    verbtaim_spec_df[\"llm_accuracy\"].values,\n",
                            "    alpha=0.05,\n",
                            "    verbose=True\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 148,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "Score_A avg: 0.354\n",
                                          "Score_B avg: 0.263\n",
                                          "Delta (B - A): -0.091\n",
                                          "p: 0.0 (threshold = 0.05)\n",
                                          "Significant\n"
                                   ]
                            },
                            {
                                   "data": {
                                          "text/plain": [
                                                 "np.True_"
                                          ]
                                   },
                                   "execution_count": 148,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "is_significantly_different(\n",
                            "    propmend_non_verbtaim_spec_df[\"llm_accuracy\"].values,\n",
                            "    non_verbtaim_spec_df[\"llm_accuracy\"].values,\n",
                            "    alpha=0.05,\n",
                            "    verbose=True\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "markdown",
                     "metadata": {},
                     "source": [
                            "### Calculate EM in RippleEdits' fashion"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 21,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "new_df_content = []\n",
                            "\n",
                            "for r_i, r in df.iterrows():\n",
                            "    new_r = deepcopy(r)\n",
                            "    if bos in r[edit_column_name]:\n",
                            "        edit_input = r[edit_column_name][r[edit_column_name].index(bos) + len(bos):r[edit_column_name].index(eos)]\n",
                            "    else:\n",
                            "        edit_input = r[edit_column_name]\n",
                            "    if original_mend:\n",
                            "        edit_input += \".\"\n",
                            "    if ice:\n",
                            "        question = r[\"question\"]\n",
                            "        icl_edit = \"Imagine that \" + edit_input[0].lower() + edit_input[1:]\n",
                            "        assert icl_edit in question, r\n",
                            "        question = question.replace(icl_edit, \"\", 1).strip()\n",
                            "    else:\n",
                            "        question = r[\"question\"]\n",
                            "    assert question == queries[r_i][\"prompt\"]\n",
                            "    answer_full_sent = queries[r_i][\"prompt\"] + \" \" + str(r[\"predicted_answer\"])\n",
                            "    new_r[\"rippleedit_exact_match\"] = float(any(a in answer_full_sent for a in queries[r_i][\"answers\"]))\n",
                            "    new_df_content.append(new_r)\n",
                            "new_df = pd.DataFrame(new_df_content)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 22,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "efficacy_Logical_Generalization (n=230.0)\n",
                                          "13.5\n",
                                          "\n",
                                          "efficacy_Compositionality_I (n=1679.0)\n",
                                          "12.4\n",
                                          "\n",
                                          "efficacy_Compositionality_II (n=273.0)\n",
                                          "59.0\n",
                                          "\n",
                                          "efficacy_Subject_Aliasing (n=777.0)\n",
                                          "77.9\n",
                                          "\n",
                                          "specificity_Relation_Specificity (n=1982.0)\n",
                                          "20.1\n",
                                          "\n",
                                          "specificity_Forgetfulness (n=282.0)\n",
                                          "47.5\n",
                                          "\n"
                                   ]
                            }
                     ],
                     "source": [
                            "df = new_df\n",
                            "for tag in [\n",
                            "        \"efficacy_Logical_Generalization\",\"efficacy_Compositionality_I\",\"efficacy_Compositionality_II\",\"efficacy_Subject_Aliasing\", \"specificity_Relation_Specificity\", \"specificity_Forgetfulness\"\n",
                            "    ]:\n",
                            "    agg = df[df[\"question_tag\"] == tag].describe()[[\"rippleedit_exact_match\",]]\n",
                            "    print(tag, f\"(n={agg['rippleedit_exact_match']['count']})\")\n",
                            "    \n",
                            "    print((agg['rippleedit_exact_match']['mean'] * 100).round(1)) #\n",
                            "    print()"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 23,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "verbtaim efficacy (RE_exact_match):  69.7\n",
                                          "non-verbtaim efficacy (RE_exact_match):  3.1\n"
                                   ]
                            }
                     ],
                     "source": [
                            "print(\"verbtaim efficacy (RE_exact_match): \",  (verbtaim_df.describe()[\"rippleedit_exact_match\"][\"mean\"] * 100).round(1))\n",
                            "print(\"non-verbtaim efficacy (RE_exact_match): \",  (non_verbtaim_df.describe()[\"rippleedit_exact_match\"][\"mean\"] * 100).round(1))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 24,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "verbtaim specificity (RE_exact_match):  97.0\n",
                                          "non-verbtaim specificity (RE_exact_match):  17.8\n"
                                   ]
                            }
                     ],
                     "source": [
                            "print(\"verbtaim specificity (RE_exact_match): \",  (verbtaim_spec_df.describe()[\"rippleedit_exact_match\"][\"mean\"] * 100).round(1))\n",
                            "print(\"non-verbtaim specificity (RE_exact_match): \",  (non_verbtaim_spec_df.describe()[\"rippleedit_exact_match\"][\"mean\"] * 100).round(1))"
                     ]
              }
       ],
       "metadata": {
              "kernelspec": {
                     "display_name": "cpt",
                     "language": "python",
                     "name": "python3"
              },
              "language_info": {
                     "codemirror_mode": {
                            "name": "ipython",
                            "version": 3
                     },
                     "file_extension": ".py",
                     "mimetype": "text/x-python",
                     "name": "python",
                     "nbconvert_exporter": "python",
                     "pygments_lexer": "ipython3",
                     "version": "3.11.0"
              }
       },
       "nbformat": 4,
       "nbformat_minor": 2
}
